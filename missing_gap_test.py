#!/usr/bin/env python3
#
# Copyright 2020 Seth Troisi
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import math
import multiprocessing
import re
import time

import gmpy2
import sqlite3
import tqdm

import gap_utils


def get_arg_parser():
    parser = argparse.ArgumentParser(
        "Test possible missing prime gaps generated by gap_stats")

    parser.add_argument('--save-logs', action='store_true',
        help="Save output to .log file")

    parser.add_argument("-t", "--threads", type=int, default=1, nargs="?",
        help="Number of threads to test with (default: %(default)s)")

    parser.add_argument('--search-db', type=str,
        default="prime-gap-search.db",
        help="Prime database from gap_test (default: %(default)s)")

    parser.add_argument("--unknown-filename", type=str, required=True,
        help="mstart, minc, p, d, sieve-length, and sieve-range"
             " are determined from filename")

    parser.add_argument("-i", "--ignore-gaps", type=int,
        nargs="*", metavar="GAPS", default=[],
        help="Ignore these gap sizes")

    return parser


def parse_unknown_lines(args, finished):
    skipped = 0
    with open(args.unknown_filename) as unknown_file:
        p_last = 1.0
        processed = []
        for line in tqdm.tqdm(unknown_file.readlines()):
            line = line.strip()
            if not line: continue

            prob, start, tests = line.split(" : ")
            prob = float(prob)

            m, p, d = parse_start(start)
            if m in finished:
                skipped += 1
                continue

            # parsing to int_tests uses 8x space,
            # runs on main thread not worker threads,
            # passing 20k character strings is fast.
            processed.append((prob, start, line.count(","), line))

            assert prob <= p_last, (prob, p_last)
            p_last = prob

    return skipped, processed


def parse_start(start):
    """
    Parse N to (m,p,d)

    '123 * 503# / 210' => (123, 503, 210)
    '123*503#/210' => (123, 503, 210)
    """
    match = re.match(r"(\d+) *\*(\d+)# *\/ *(\d+)", start)
    assert match
    return map(int, match.groups())


def print_count_timing(s_start_t, prefix, count):
    secs = time.time() - s_start_t

    # Want 3 sig figs which is hard in python
    def roundSig(n, sig):
        return '{:g}'.format(float('{:.{p}g}'.format(n, p=sig)))

    if count and count < secs:
        timing = "{} secs/test".format(roundSig(secs / count, 3))
    else:
        timing = "{}/sec".format(roundSig(count / secs, 3))

    print("\t{:12} {:10d} tests ({})  {:.0f} seconds elapsed".format(
        prefix, count, timing, secs))


def load_existing(conn, args):
    # TODO: min_merit and num_to_process...
    rv = list(conn.execute(
        "SELECT rid, min_merit, num_to_processed FROM range"
        " WHERE P = ? AND D = ? AND m_start = ? AND m_inc = ?",
        (args.p, args.d, args.mstart, args.minc)))
    assert len(rv) == 1, rv
    rid = rv[0][0]

    rv = conn.execute(
        "SELECT m FROM m_missing_stats"
        " WHERE P = ? and D = ? and m BETWEEN ? AND ?",
        (args.p, args.d, args.mstart, args.mstart + args.minc - 1))
    return rid, set(row['m'] for row in rv)


def save_record(
        conn, rid, m, p, d,
        prob, merit, prev_p, next_p,
        prev_tests, next_tests, test_time):
    # TODO: verify next,prev and see if they can match schema.sql
    # TODO: support next_p, prev_p conventions
    conn.execute(
        "INSERT INTO m_missing_stats"
        "   (rid,m,P,D,"
        "    next_p,prev_p,"
        "    merit,prob_record,prob_missing,prob_merit,"
        "    e_gap_next,e_gap_prev,"
        "    prp_next,prp_prev,"
        "    test_time) VALUES"
        "   (?,?,?,?,  ?,?,   ?,0,?,0,  0,0,  ?,?,?)",
        (rid, m, p, d,
         next_p, prev_p,
         merit, prob,
         next_tests, prev_tests,
         test_time))
    conn.commit()


# ----- Worker

# Makes tested/second much more stable
g_tested = multiprocessing.Value('i', 0)

def test_and_increment(p, start, str_diff):
    # counting before test seems to lead to stabler primes/second
    with g_tested.get_lock(): g_tested.value += 1
    return gap_utils.is_prime(p, start, str_diff)

# See Pool and initializer which set test_line.ignore_gaps
def init_worker(function, ignore):
    function.ignore_gaps = ignore


def test_line(data):
    prob, start, test_count, tests = data

    m, p, d = parse_start(start)
    #assert p == str(P) and d == str(D), (p, d, P, D)

    K, r = divmod(gmpy2.primorial(p), d)
    assert r == 0
    N = m * K

    cache = {}

    skipped = 0
    max_high = None
    max_low  = None

    t0 = time.time()

    for low, high in re.findall("(\d+),(\d+)", tests):
        low, high = int(low), int(high)
        assert low > 0 and high > 0

        if max_low and low > max_low:
            continue

        if max_high and high > max_high:
            continue

        if (low + high) in test_line.ignore_gaps:
            skipped += 1
            continue

        # use - for low so that (N - x) doesn't conflict with (N + x)
        if -low not in cache:
            cache[-low] = test_and_increment(N - low, start, "-" + str(low))
            if cache[-low]:
                # TECHNICALLY Should never have more than one success:
                # if low,high happen in reasonable order after the first pair
                # all intervals will be larger (and invalid).
                assert max_low is None
                max_low = low

        if not cache[-low]:
            continue

        if high not in cache:
            cache[high] = test_and_increment(N + high, start, "+" + str(high))
            if cache[high]:
                # See TECHNICALLY note above.
                assert max_high is None
                max_high = high

    t1 = time.time()

    tested_low  = sum(1 for i in cache if i < 0)
    tested_high = sum(1 for i in cache if i > 0)
    primes = sum(cache.values())

    return (
        # input data copied to output
        prob, start, test_count,
        # top level output counters
        skipped, primes, tested_low, tested_high, t1 - t0,
        # primes found
        max_low, max_high
    )

def prime_gap_test(args):

    # ----- Open missing gap input file
    print("Loading unknowns from '{}'".format(args.unknown_filename))

    print("Loading finished records from '{}'".format(args.search_db))

    conn = sqlite3.connect(args.search_db)
    conn.row_factory = sqlite3.Row
    rid, finished = load_existing(conn, args)
    if finished:
        print("\tFound {} already processed(m {} to {})".format(
            len(finished), min(finished), max(finished)))

    print()

    if args.ignore_gaps:
        print ("\tSkipping gaps: {}\n".format(
            ", ".join(map(str, args.ignore_gaps))))

    K, K_digits, K_bits, K_log = gap_utils.K_and_stats(args)
    print("K = {} bits, {} digits, log(K) = {:.2f}\n".format(
        K_bits, K_digits, K_log))

    # Used for various stats
    s_start_t = time.time()
    s_last_print_t = time.time()
    s_last_success_count = 0

    successes = []
    summed_prob = 0
    skipped_prob = 0
    skipped = 0
    tested = 0
    primes = 0

    skipped_lines, lines = parse_unknown_lines(args, finished)
    if len(lines) == 0:
        print("All {} lines already processed".format(skipped_lines))
        # Could have processed other m from other runs.
        assert skipped_lines <= len(finished)
        exit(1)

    print("Read {} lines ({} already processed) with missing_gap_prob: {:.3g} to {:.3g}".format(
        len(lines), skipped_lines, lines[0][0], lines[-1][0]))

    probs = [line[0] for line in lines]
    assert max(probs) == lines[0][0], "Out of order lines!"
    assert min(probs) == lines[-1][0], "Out of order lines!"

    # Pass *init_args to initializer
    init_args = (test_line, args.ignore_gaps)

    with multiprocessing.Pool(args.threads, init_worker, init_args) as pool:
        for li, (prob, start, test_count, line_s, line_p,
                 test_low, test_high, test_time, prev_p, next_p) in \
                enumerate(pool.imap_unordered(test_line, lines)):

            # XXX: load gaps.db and verify this is a missing gap?
            if prev_p and next_p:
                succ = "BOTH PRIME: {} -{} to +{}".format(start, prev_p, next_p)
                successes.append(succ)
                print("\n"*3)
                print("\t", succ)
                print("\n"*3)

            m, p, d = parse_start(start)
            merit = (prev_p + next_p) / (K_log + math.log(m)) if prev_p and next_p else 0
            save_record(
                conn, rid, m, p, d, prob,
                merit, prev_p or 0, next_p or 0,
                test_low, test_high, test_time)

            ratio = 1 - line_s / test_count
            summed_prob += prob * ratio
            skipped += line_s
            tested  += test_low + test_high
            primes  += line_p

            s_stop_t = time.time()
            secs = s_stop_t - s_start_t
            print_secs = s_stop_t - s_last_print_t

            if line_s == 0:
                pair_print = "{:<4}".format(test_count)
                prob_print   = "{:.2e}".format(prob)
            else:
                pair_print = "{:4}/{:<4}".format(test_count - line_s, test_count)
                prob_print   = "{:.2e}/{:.2e}".format(ratio * prob, prob)

            print("finished: {} ({} from {} pairs)\t|".format(
                start, prob_print, pair_print, test_count),
                end = " ")

            li1 = li + 1

            # Adjusted a little to make `per_day` more stable
            avg_test = secs / li1
            adj_secs = max(avg_test, secs - (avg_test * args.threads / 2))
            per_day = "  per day: {:.3f}".format(summed_prob / (adj_secs / 86400))

            print("m: {}/{}, p/t: {}/{}, sum(prob): {:.4f}{}".format(
                li + 1, len(lines),
                primes, tested,
                summed_prob, per_day if 2 * li >= args.threads else ""))

            if li1 in (1,2,5,10,20,50, len(lines)) or li1 % 100 == 0 or print_secs > 240:
                s_last_print_t = s_stop_t

                # TODO: eta for finishing
                with g_tested.get_lock():
                    print_count_timing(s_start_t, li1, g_tested.value)

                if len(successes) - s_last_success_count >= 5:
                    print("\n"*2)
                    for success in successes[s_last_success_count:]:
                        print("\t", success)
                    print("\n"*2)
                    s_last_success_count = len(successes)

    print_count_timing(s_start_t, "END: " + str(li + 1), tested)

    print("\n"*2)
    print("All successes ({} from {} prob):".format(
        len(successes), summed_prob))
    for success in successes:
        print("\t", success)
    print("\n"*2)


if __name__ == "__main__":
    parser = get_arg_parser()
    args = parser.parse_args()
    gap_utils.verify_args(args, ".missing")

    # TeeLogger context if args.save_logs
    with gap_utils.logger_context(args):
        prime_gap_test(args)

